import streamlit as st
from typing import Dict, Tuple

@st.cache_resource(show_spinner=True)
def load_model(model_name: str):
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    return tokenizer, model

def predict(text: str, model_name: str, temperature: float = 1.0) -> Tuple[float, float, Dict[str, float]]:
    import torch
    from torch.nn.functional import softmax
    tokenizer, model = load_model(model_name)
    # roberta é¡æ¨¡å‹æœ€å¤§åºåˆ—é•·åº¦é€šå¸¸ç‚º 512
    encoded = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.inference_mode():
        logits = model(**encoded).logits
        if temperature and temperature > 0 and temperature != 1.0:
            logits = logits / temperature
        probs = softmax(logits, dim=-1)[0].cpu().numpy().tolist()

    # ä½¿ç”¨æ¨¡å‹çš„ id2label æä¾›çš„æ¨™ç±¤åç¨±ï¼Œé¿å…èª¤åˆ¤
    # roberta-base-openai-detector é€šå¸¸ç‚º {0: 'Real', 1: 'Fake'}
    id2label = getattr(model.config, 'id2label', {0: 'Real', 1: 'Fake'})
    labels = [id2label.get(i, f"LABEL_{i}").lower() for i in range(len(probs))]

    # å˜—è©¦ä»¥åç¨±å°æ‡‰ï¼Œè‹¥åç¨±ç¼ºå¤±å‰‡ä»¥ç´¢å¼•å›é€€
    real_idx = labels.index('real') if 'real' in labels else 0
    fake_idx = labels.index('fake') if 'fake' in labels else (1 if len(probs) > 1 else 0)

    real = float(probs[real_idx])
    fake = float(probs[fake_idx])
    raw = {labels[i]: float(probs[i]) for i in range(len(probs))}
    return real, fake, raw

def render_metrics(real: float, fake: float):
    col1, col2 = st.columns(2)
    col1.metric("äººé¡æ–‡æœ¬æ©Ÿç‡", f"{real*100:.2f}%")
    col2.metric("AI æ–‡æœ¬æ©Ÿç‡", f"{fake*100:.2f}%")
    st.progress(int(fake * 100))
    st.caption("é€²åº¦æ¢ä»¥ AI æ©Ÿç‡è¡¨ç¤º")

def render_pie(real: float, fake: float):
    try:
        import matplotlib.pyplot as plt
        fig, ax = plt.subplots()
        ax.pie([real, fake], labels=["äººé¡", "AI"], autopct='%1.1f%%', colors=['#4CAF50', '#FF7043'])
        ax.axis('equal')
        st.pyplot(fig)
    except Exception:
        st.info("åœ–è¡¨æ¨¡çµ„ä¸å¯ç”¨ï¼Œç•¥éåœ“é¤…åœ–ã€‚")

def main():
    st.set_page_config(page_title="AI å…§å®¹åµæ¸¬å™¨", page_icon="ğŸ¤–", layout="centered")
    st.title("AI å…§å®¹åµæ¸¬å™¨")
    st.write("è¼¸å…¥ä¸€æ®µæ–‡å­—ï¼Œç³»çµ±å°‡åˆ¤æ–·å…¶ç‚ºäººé¡æ’°å¯«æˆ– AI ç”Ÿæˆçš„å¯èƒ½æ€§ã€‚")

    # æ¨¡å‹é¸æ“‡èˆ‡è¨ºæ–·åƒæ•¸
    with st.sidebar:
        st.header("æ¨¡å‹èˆ‡è¨­å®š")
        model_name = st.selectbox(
            "é¸æ“‡åµæ¸¬æ¨¡å‹",
            options=[
                "roberta-base-openai-detector",
                # å¯å†åŠ å…¥å…¶ä»–æª¢æ¸¬æ¨¡å‹ ID
            ],
            index=0,
        )
        temperature = st.slider("Softmax æº«åº¦ (æ ¡æ­£)", min_value=0.5, max_value=2.0, value=1.0, step=0.1,
                                help=">1 é™ä½æ¥µç«¯ä¿¡å¿ƒï¼Œ<1 æé«˜å°–éŠ³åº¦ã€‚")
        threshold = st.slider("AI åˆ¤å®šé–¾å€¼ (Fake)", min_value=0.5, max_value=0.9, value=0.6, step=0.05,
                               help="ç•¶ AI æ©Ÿç‡ â‰¥ é–¾å€¼ æ™‚åˆ¤å®šç‚º AI ç”Ÿæˆã€‚")

    # ç¯„ä¾‹æ–‡æœ¬ï¼ˆè‹±æ–‡ç¤ºä¾‹ï¼‰
    demo_ai = (
        "In modern machine learning systems, generalization performance often depends on data distribution,\n"
        "regularization strategies, and trade-offs in multi-objective optimization to achieve robust metrics."
    )
    demo_human = (
        "This morning my commute took about half an hour, so I finished yesterday's article on the train.\n"
        "When I got to the office, I cleaned up my to-do list and fixed a small bug that had been blocking me."
    )

    with st.expander("å¿«é€Ÿå¥—ç”¨ç¯„ä¾‹æ–‡æœ¬"):
        c1, c2 = st.columns(2)
        if c1.button("å¥—ç”¨ AI ç”Ÿæˆç¤ºä¾‹ï¼ˆè‹±æ–‡ï¼‰"):
            st.session_state["input_text"] = demo_ai
        if c2.button("å¥—ç”¨äººé¡æ’°å¯«ç¤ºä¾‹ï¼ˆè‹±æ–‡ï¼‰"):
            st.session_state["input_text"] = demo_human

    default_text = st.session_state.get("input_text", "")
    text = st.text_area("è«‹è²¼ä¸Šå¾…åˆ†ææ–‡æœ¬", value=default_text, height=240, help="æ”¯æ´é•·æ–‡è¼¸å…¥ï¼Œè«‹æŒ‰ä¸‹æ–¹æŒ‰éˆ•é–‹å§‹åˆ†æã€‚")
    analyze = st.button("é–‹å§‹åˆ†æ")

    if analyze:
        if not text or not text.strip():
            st.error("è¼¸å…¥ä¸å¯ç‚ºç©ºï¼Œè«‹æä¾›æ–‡æœ¬å¾Œé‡è©¦ã€‚")
            return
        # èªè¨€åµæ¸¬ï¼ˆæç¤ºæ¨¡å‹é©ç”¨æ€§ï¼‰
        try:
            from langdetect import detect
            lang = detect(text.strip())
            if lang != 'en':
                st.info("åµæ¸¬åˆ°éè‹±æ–‡æ–‡æœ¬ã€‚ç•¶å‰æ¨¡å‹ä¸»è¦é‡å°è‹±æ–‡è¨“ç·´ï¼Œçµæœå¯èƒ½å¤±æº–ï¼Œå»ºè­°æ›´æ›ç‚ºæ”¯æ´ä¸­æ–‡çš„åµæ¸¬æ¨¡å‹æˆ–èª¿æ•´åˆ¤å®šé–¾å€¼ã€‚")
        except Exception:
            pass
        with st.spinner("è¼‰å…¥æ¨¡å‹ä¸¦åˆ†æä¸­â€¦"):
            try:
                real, fake, raw = predict(text.strip(), model_name=model_name, temperature=temperature)
            except Exception as e:
                st.error(f"åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                return
        # è‹¥æ–‡æœ¬éé•·ï¼Œæç¤ºå·²æˆªæ–·
        try:
            from transformers import AutoTokenizer
            tk = AutoTokenizer.from_pretrained(model_name)
            if len(tk(text).get('input_ids', [])) > 512:
                st.warning("æ–‡æœ¬è¶…éæ¨¡å‹æœ€å¤§é•·åº¦ï¼Œå·²è‡ªå‹•æˆªæ–·è‡³å‰ 512 tokenï¼Œçµæœå¯èƒ½å—å½±éŸ¿ã€‚")
        except Exception:
            pass

        # çµæœé¡¯ç¤º
        verdict = "AI ç”Ÿæˆ" if fake >= threshold else "äººé¡æ’°å¯«"
        confidence = max(real, fake)
        st.subheader("åˆ†æçµæœ")
        st.success(f"åˆ¤å®šï¼š{verdict}ï¼ˆä¿¡å¿ƒ {confidence*100:.2f}%ï¼‰")

        render_metrics(real, fake)
        st.divider()
        st.caption("æ¯”ä¾‹è¦–è¦ºåŒ–ï¼ˆå¯é¸ï¼‰")
        render_pie(real, fake)

        # é¡¯ç¤ºåŸå§‹æ¨™ç±¤èˆ‡æ©Ÿç‡ä»¥ä¾¿è¨ºæ–·
        st.divider()
        st.subheader("è¨ºæ–·ï¼šåŸå§‹æ¨™ç±¤èˆ‡æ©Ÿç‡")
        st.table({"label": list(raw.keys()), "prob": [f"{p*100:.2f}%" for p in raw.values()]})

if __name__ == "__main__":
    main()